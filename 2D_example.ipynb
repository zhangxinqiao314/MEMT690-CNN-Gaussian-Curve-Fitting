{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93lN34CEhk6A",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8657,
     "status": "ok",
     "timestamp": 1669491618857,
     "user": {
      "displayName": "Xinqiao Zhang",
      "userId": "17385836557717323900"
     },
     "user_tz": 300
    },
    "id": "vItZjqpEc4rF",
    "outputId": "cec64736-b6f6-4a8a-ca6e-18a979973d93"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/m3-learning/DeepMatter.git\n",
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 261790,
     "status": "ok",
     "timestamp": 1669492764296,
     "user": {
      "displayName": "Xinqiao Zhang",
      "userId": "17385836557717323900"
     },
     "user_tz": 300
    },
    "id": "M2OjqSfBf3lJ",
    "outputId": "7eb789f4-1e37-493e-8254-8a4b6e375989"
   },
   "outputs": [],
   "source": [
    "!pip install DeepMatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PWcV7qehwRw"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 108,
     "status": "ok",
     "timestamp": 1669492908824,
     "user": {
      "displayName": "Xinqiao Zhang",
      "userId": "17385836557717323900"
     },
     "user_tz": 300
    },
    "id": "_p2oEJVZc4rI"
   },
   "outputs": [],
   "source": [
    "# import DeepMatter as dm\n",
    "# from DeepMatter.spectral_fitters.gaussian import Gaussian\n",
    "# # from DeepMatter.spectral_fitters.voigt import PseudoVoigt\n",
    "# from DeepMatter.spectral_fitters import nn\n",
    "# from DeepMatter.rand_util.rand_gen import rand_tensor\n",
    "# # from DeepMatter.spectral_fitters.nn import DensePhysEnc9185\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import time\n",
    "import math\n",
    "# from DeepMatter.util.torch_util import Dataset_Generator\n",
    "\n",
    "from pdb import set_trace as bp\n",
    "from tqdm import tqdm as tqdm\n",
    "from datetime import date\n",
    "\n",
    "from ipywidgets import interact\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_tensor(min=0, max=1, size=(1)):\n",
    "    \"\"\" Function that generates random tensor between a range of an arbitary size\n",
    "\n",
    "    :param min:  sets the minimum value of parameter\n",
    "    :type min: float\n",
    "    :param max:  sets the maximum value of parameter\n",
    "    :type max: float\n",
    "    :param size: sets the size of the random vector to generate\n",
    "    :type size: tuple\n",
    "    :return: random tensor generated\n",
    "    :rtype: tensor\n",
    "    \"\"\"\n",
    "\n",
    "    out = (max - min) * torch.rand(size,dtype=torch.float32) + min\n",
    "    return out.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset_Generator(Dataset):\n",
    "    \"\"\"\n",
    "    Function that generates data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 samples_per_epoch=10000,\n",
    "                 size=1,\n",
    "                 batch_size=32,\n",
    "                 **kwargs):\n",
    "        self.model = model\n",
    "        self.samples_per_epoch = samples_per_epoch\n",
    "        self.sd = kwargs.get('sd')\n",
    "        self.mean = kwargs.get('mean')\n",
    "        self.amp = kwargs.get('amp')\n",
    "        self.fraction = kwargs.get('fraction')\n",
    "        self.x_vector = kwargs.get('x_vector') # always assume a square\n",
    "        self.size = size\n",
    "        self.batch_size = batch_size\n",
    "        self.noise = kwargs.get('noise')\n",
    "        self.verbose = kwargs.get('verbose')\n",
    "        self.function = self.model(self.x_vector,\n",
    "                                   sd=self.sd,\n",
    "                                   mean=self.mean,\n",
    "                                   amp=self.amp,\n",
    "                                   size=self.size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples_per_epoch\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.function.batch_size=self.batch_size\n",
    "        input_, params = self.function.sampler(device='cuda')\n",
    "        \n",
    "        if self.noise is not None:\n",
    "            # if self.verbose is True and not None:\n",
    "                # print(\"adding noise\\n\")\n",
    "            input_ += np.random.uniform(0, self.noise, size=input_.shape)\n",
    "        return {'input': input_, 'params': params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_tensor(tensor):\n",
    "    '''\n",
    "    tensor: a tensor to plot\n",
    "    indices: shape tensor[:,d,:,v] give you the values to plot\n",
    "    '''\n",
    "    plt.imshow(tensor.T.cpu().detach().numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## pseudovoigt\n",
    "* The voigt function is a probability distribution given by a convolution of a Cauchy-Lorentz distribution and a Gaussian distribution. It is often used in analyzing data from spectroscopy or diffraction. \n",
    "* ${\\displaystyle V(x;\\sigma ,\\gamma )\\equiv \\int _{-\\infty }^{\\infty }G(x';\\sigma )L(x-x';\\gamma )\\,dx',}$ where\n",
    "  * ${\\displaystyle G(x;\\sigma )\\equiv {\\frac {e^{-x^{2}/(2\\sigma ^{2})}}{\\sigma {\\sqrt {2\\pi }}}},}$ and \n",
    "  * ${\\displaystyle L(x;\\gamma )\\equiv {\\frac {\\gamma }{\\pi (x^{2}+\\gamma ^{2})}}.}$\n",
    "  \n",
    "  \n",
    "* we are implementing the pseudovoigt approximation, which uses a linear combination of the gaussian and lorentzan, instead of the convolution. This is much faster to calculate\n",
    "* The model is bsed off of the methods found [here](https://docs.mantidproject.org/nightly/fitting/fitfunctions/PseudoVoigt.html)\n",
    "enter image description hereenter image description here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1669492958994,
     "user": {
      "displayName": "Xinqiao Zhang",
      "userId": "17385836557717323900"
     },
     "user_tz": 300
    },
    "id": "q4p_MzR3c4rI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PseudoVoigt_2D:\n",
    "    \"\"\"\n",
    "    Class that computes the voigt (distribution) of a batch of data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x_vector,\n",
    "                 sd=[[0,1],[0,1]],\n",
    "                 mean=[[0,1],[0,1]],\n",
    "                 amp=[[0,1],[0,1]],\n",
    "                 fraction=[[0,1],[0,1]],\n",
    "                 size=(1, 1),\n",
    "                 verbose=False):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x_vector:\n",
    "            sd (array, float): x,y ranges for the standard deviation\n",
    "            mean (array, float): ranges for the mean\n",
    "            amp (array, float): ranges for the amplitude\n",
    "            size (tuple): Size of the array first index is number of channels, second is number of functions\n",
    "            verbose (bool): shows outputs\n",
    "        \"\"\"\n",
    "\n",
    "        self.x_vector = x_vector\n",
    "        num_params = 4 * 3 # 12 because the x-y axis are not correlated. will implement different sd function in future\n",
    "        \n",
    "        self.sd = sd # fwhm/2 of profile\n",
    "        self.sd_mean = torch.sum(torch.tensor(sd),dim=0) / 2\n",
    "        self.sd_sd = torch.sqrt((torch.pow(torch.tensor(sd[:][1]) - torch.tensor(sd[:][0]), 2) / num_params)) # 12 bc 3humps * 4params \n",
    "        # bp()\n",
    "        self.mean = mean\n",
    "        self.mean_mean = torch.sum(torch.tensor(mean),dim=0) / 2\n",
    "        self.mean_sd = torch.sqrt((torch.pow(torch.tensor(mean[:][1]) - torch.tensor(mean[:][0]), 2) / num_params))\n",
    "        \n",
    "        self.amp = amp\n",
    "        self.amp_mean = torch.sum(torch.tensor(amp),dim=0) / 2\n",
    "        self.amp_sd = torch.sqrt((torch.pow(torch.tensor(amp[:][1]) - torch.tensor(amp[:][0]), 2) / num_params))\n",
    "        \n",
    "        self.fraction = fraction # (1-eta), which determines if peak is more gaussian (0) or more lorentzian (1)\n",
    "        self.fraction_mean = torch.sum(torch.tensor(fraction),dim=0) / 2\n",
    "        self.fraction_sd = torch.sqrt((torch.pow(torch.tensor(fraction[:][1]) - torch.tensor(fraction[:][0]), 2) / num_params))\n",
    "        \n",
    "        self.size = size\n",
    "        self.verbose = verbose\n",
    "        self.x,self.y = torch.meshgrid(x_vector,x_vector)\n",
    "        self.dst = torch.sqrt(self.x**2+self.y**2)\n",
    "\n",
    "        # bp()\n",
    "        \n",
    "        \n",
    "    def gauss_pdf_2D(self,x,y,mean,sd,amp):\n",
    "        '''\n",
    "        x,y: from torch.meshgrid\n",
    "        mean: [mx,my]\n",
    "        sd: [sx,sy]\n",
    "        '''\n",
    "        \n",
    "        mx=mean[:,0].reshape(-1,1,1)\n",
    "        my=mean[:,1].reshape(-1,1,1)\n",
    "        sx=sd[:,0].reshape(-1,1,1)\n",
    "        sy=sd[:,1].reshape(-1,1,1)\n",
    "        pi_ = torch.tensor(math.pi).reshape(-1,1,1)\n",
    "        \n",
    "        distr = amp*torch.exp(-((x - mx)**2 / (2*sx**2) + (y - my)**2 / (2*sy**2)))\n",
    "        # bp()\n",
    "        return distr\n",
    "        \n",
    "    def compute(self, params, device='cpu', noise=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            self (object): Returns the instance itself.\n",
    "            params: for computation --> shape (1,1,2,4,3) #ch, bsize, dims, # par, # humps\n",
    "            device (string, optional) : Sets the device to do the computation. Default `cpu`, common option `cuda`\n",
    "\n",
    "        Returns: (sum(out), out) (list of Tensor): spectra added together, and individually\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if self.verbose:\n",
    "            print({f'pre-param size {params.size()}'})\n",
    "\n",
    "        if len(params.size()) == 2:\n",
    "            print('look')\n",
    "            params = torch.reshape(params, (params.shape[0], 4, -1))\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f'parm size = {params.size()}')\n",
    "            print(f'x_vector size = {self.x_vector.shape[0]}')\n",
    "\n",
    "        # out.shape = (bsize, xlen ,ylen , #ch, #humps)\n",
    "        out = torch.zeros((params.shape[0],\n",
    "                           self.x_vector.shape[0],\n",
    "                           self.x_vector.shape[0],\n",
    "                           self.size[0],\n",
    "                           self.size[1]))\n",
    "        if self.verbose:\n",
    "            print(self.size[1])\n",
    "        # params.shape = [1, 2, 4, 3] (bsize,dims,params,humps)\n",
    "        params = params.to(device)\n",
    "        \n",
    "        x,y = self.x.to(device),self.y.to(device)\n",
    "        x,y = x.repeat(params.shape[0],1,1),y.repeat(params.shape[0],1,1)\n",
    "        # bp()\n",
    "        for i in range(self.size[1]):\n",
    "            if params.ndim == 5: # if it's a whole batch\n",
    "                _mean = params[:,0,:, 0, i] \n",
    "                _sd = params[:, 0,:, 1, i] \n",
    "                _amp = params[:, 0,:, 2, i]  \n",
    "                # _fraction = params[:, 0,:,3, i] \n",
    "\n",
    "            if params.ndim == 4: # if its a single value\n",
    "                _mean = params[:,:, 0, i] \n",
    "                _sd = params[:, :, 1, i] \n",
    "                _amp = params[:,:,2, i]  \n",
    "                # _fraction = params[:, :,3, i] \n",
    "                \n",
    "            # x_vector = torch.cat(params.shape[0] * [self.x_vector]).reshape(params.shape[0],2,-1).to(device) # (ch,dims,12params)\n",
    "            # x_vector = torch.transpose(x_vector,0,1)\n",
    "            # x_vector = torch.transpose(x_vector,1,2)\n",
    "            # (dims,12params,ch)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f'x_vector_shape = {x_vector.size()}')\n",
    "            \n",
    "            # Get gaussian distribution\n",
    "            # bp()\n",
    "            _out = self.gauss_pdf_2D(x,y,_mean,_sd,_amp[0,0])\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f'amp {_amp.size()}, base {_base.size()}, exp {_exp.size()}')\n",
    "                print(f'out shape = {_out.shape}')\n",
    "            # bp()\n",
    "    \n",
    "            out[:,:,:,0, i] = _out\n",
    "        out = torch.transpose(out,1,3).type(torch.FloatTensor)\n",
    "        # out.shape = (bsize, xlen ,ylen , #ch, #humps)\n",
    "        \n",
    "        return torch.sum(out, dim=-1).squeeze(dim=0)#, out\n",
    "    \n",
    " \n",
    "    def sampler(self,device='cpu',noise=0):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            device (str): device where computation happens\n",
    "\n",
    "        Returns:\n",
    "            out (Tensor) : Generated spectra\n",
    "            params (Tensor) : parameters used for generation\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Shape (2,1,3)\n",
    "        sd = torch.stack([   rand_tensor(min=self.sd[0][0], max=self.sd[0][1], \n",
    "                             size=(self.size[0],self.size[1])),\n",
    "                             rand_tensor(min=self.sd[1][0], max=self.sd[1][1], \n",
    "                             size=(self.size[0],self.size[1])) ])\n",
    "        \n",
    "        mean = torch.stack([ rand_tensor(min=self.mean[0][0], max=self.mean[0][1], \n",
    "                             size=(self.size[0],self.size[1])),\n",
    "                             rand_tensor(min=self.mean[1][0], max=self.mean[1][1], \n",
    "                             size=(self.size[0],self.size[1])) ])\n",
    "        \n",
    "        amp = torch.stack([  rand_tensor(min=self.amp[0], max=self.amp[1], \n",
    "                             size=(self.size[0],self.size[1])),\n",
    "                             rand_tensor(min=self.amp[0], max=self.amp[1], \n",
    "                             size=(self.size[0],self.size[1])) ])\n",
    "\n",
    "        fraction = torch.stack([ rand_tensor(min=self.fraction[0][0], max=self.fraction[0][1], \n",
    "                                 size=(self.size[0],self.size[1])),\n",
    "                                 rand_tensor(min=self.fraction[1][0], max=self.fraction[1][1], \n",
    "                                 size=(self.size[0],self.size[1])) ])\n",
    "        \n",
    "        # fraction = torch.tensor([[[0],[0]],[[0],[0]]]).type(torch.FloatTensor)\n",
    "\n",
    "        _params = torch.torch.stack((mean, sd, amp, fraction)) # (4,2,1,3)\n",
    "        _params = torch.atleast_3d(_params) # (same)\n",
    "        _params = torch.transpose((_params), 0, 2) # --> (2,1,4,3) #ch, dims, bsize, #parameters, #humps\n",
    "        # _params = torch.transpose((_params), 1, 2) # --> (1,2,4,3) #ch, bsize, dims, # par, # humps\n",
    "        #return _params,_params\n",
    "\n",
    "        #returns the number of parameters that was calculated to generate the best results\n",
    "        # bp()\n",
    "        return self.compute(_params,device=device,noise=noise), _params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters:\n",
    "# seed=42\n",
    "noise=1\n",
    "# torch.manual_seed(seed)\n",
    "batch_size = 32\n",
    "x_vector = torch.linspace(0,10,100) #evenly distributes values from 1 to 10 with 100 steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinqiao/anaconda3/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "data_test = Dataset_Generator(model=PseudoVoigt_2D,\n",
    "                              mean=[[2,8],[5,7]], \n",
    "                              sd=[[.5,2],[1,3]],\n",
    "                              amp=[2,6], \n",
    "                              fraction = [[0,0],[0,0]],\n",
    "                              x_vector = torch.linspace(0,10,100),\n",
    "                              size=(1,3),\n",
    "                              samples_per_epoch=1000*batch_size,\n",
    "                              batch_size=batch_size,\n",
    "                              noise=noise,\n",
    "                              verbose=True)\n",
    "\n",
    "dataloader = DataLoader(data_test, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=0)\n",
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = constructor.compute(batch['params'])\n",
    "# test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input (32, 1, 100, 100)\n",
      "params (32, 1, 2, 4, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6741e327a484d59946206ca1c793a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=15, description='i', max=31), Output()), _dom_classes=('widget-interact'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.f(i)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = batch['input'].cpu().detach().numpy()\n",
    "p = batch['params'].cpu().detach().numpy()\n",
    "print('input', x.shape)\n",
    "print('params', p.shape)\n",
    "\n",
    "def f(i):\n",
    "    # print(p[i,0])\n",
    "    plt.imshow(x[i,0,:,:])\n",
    "    plt.colorbar()\n",
    "    plt.xticks(ticks = np.linspace(0,99,11),labels = np.linspace(0,10,11))\n",
    "    plt.yticks(ticks = np.linspace(0,99,11),labels = np.linspace(0,10,11))\n",
    "    plt.scatter(p[i,0,0,0]*10,p[i,0,1,0]*10,c='r',marker='x') # plot means\n",
    "    plt.show()\n",
    "    \n",
    "interact(f,i=(0,batch_size-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 100, 100])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 120,
     "status": "ok",
     "timestamp": 1669493661160,
     "user": {
      "displayName": "Xinqiao Zhang",
      "userId": "17385836557717323900"
     },
     "user_tz": 300
    },
    "id": "g2hc-feTc4rM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class DensePhysLarger(nn.Module):\n",
    "    def __init__(self,\n",
    "                 x_vector,\n",
    "                 model,\n",
    "                 dense_params=3,\n",
    "                 verbose=False,\n",
    "                 device = 'cuda',\n",
    "                 num_channels=1,\n",
    "                 **kwargs):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_vector: The vector of values for x\n",
    "            model: the empirical function to fit\n",
    "            dense_params: number of output parameters to the model\n",
    "            verbose: sets if the model is verbose\n",
    "            device: device where the model will run\n",
    "            num_channels: number of channels in the input\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.dense_params = dense_params\n",
    "        self.x_vector = x_vector\n",
    "        self.verbose = verbose\n",
    "        self.num_channels = num_channels\n",
    "        self.device = device\n",
    "        self.model_params = kwargs.get('model_params')\n",
    "        self.model = model #(self.x_vector, size=(num_channels, dense_params // self.model_params))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        n = 4\n",
    "\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "\n",
    "        # Input block of 1d convolution\n",
    "        self.hidden_x1 = nn.Sequential(\n",
    "            nn.Conv2d(self.num_channels, 8*n, kernel_size=7), nn.SELU(),\n",
    "            nn.Conv2d(8*n, 6*n, kernel_size=7), nn.SELU(),\n",
    "            nn.Conv2d(6*n, 4, kernel_size=5), nn.SELU(),\n",
    "            nn.Conv2d(4, 4, kernel_size=5), nn.SELU(),\n",
    "            nn.Conv2d(4, 4, kernel_size=3), nn.SELU(),\n",
    "            nn.Conv2d(4, 4, kernel_size=3), nn.SELU(),\n",
    "        )\n",
    "\n",
    "        self.hidden_x1_shape = self.hidden_x1( torch.zeros(1, \n",
    "                                                            self.num_channels,\n",
    "                                                            self.x_vector.shape[0],\n",
    "                                                            self.x_vector.shape[0],\n",
    "                                                            )).shape\n",
    "        # bp()\n",
    "        #fully connected block\n",
    "        self.hidden_xfc = nn.Sequential(\n",
    "            nn.Linear(self.hidden_x1_shape[1] * self.hidden_x1_shape[2]*self.hidden_x1_shape[3], 200), nn.SELU(),\n",
    "            nn.Linear(200,100), nn.SELU(),\n",
    "        )  # out of size 100\n",
    "        \n",
    "        self.hidden_xfc_shape = self.hidden_xfc(torch.zeros(1,\n",
    "                                                            self.hidden_x1_shape[1]*self.hidden_x1_shape[2]*self.hidden_x1_shape[3])).shape\n",
    "        # bp()\n",
    "        # 2nd block of 1d-conv layers\n",
    "        self.hidden_x2 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(4, 4, kernel_size=5), nn.SELU(),\n",
    "            nn.Conv2d(4, 4, kernel_size=5), nn.SELU(),\n",
    "            nn.Conv2d(4, 4, kernel_size=3), nn.SELU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(4, 4, kernel_size=3), nn.SELU(),\n",
    "            nn.Conv2d(4, 4, kernel_size=3), nn.SELU(),\n",
    "            nn.Conv2d(4, 4, kernel_size=3), nn.SELU(),\n",
    "        )\n",
    "        \n",
    "        self.hidden_x2_shape = self.hidden_x2(torch.zeros(self.hidden_xfc_shape[0],\n",
    "                                                          self.hidden_x1_shape[1],\n",
    "                                                          self.hidden_x1_shape[2],\n",
    "                                                          self.hidden_x1_shape[3]\n",
    "                                                         )).shape\n",
    "        \n",
    "        # 3nd block of 1d-conv layers\n",
    "        self.hidden_x3 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(1, 4*n, kernel_size=3), nn.SELU(),\n",
    "            nn.Conv2d(4*n, 4*n, kernel_size=3), nn.SELU(),\n",
    "            nn.Conv2d(4*n, 4*n, kernel_size=3), nn.SELU(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            nn.Conv2d(4*n, 2*n, kernel_size=3), nn.SELU(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            nn.Conv2d(2*n, 2, kernel_size=3), nn.SELU(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            nn.Conv2d(2, 1, kernel_size=3), nn.SELU(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "        )\n",
    "        \n",
    "        self.hidden_x3_shape = self.hidden_x3(torch.zeros(self.hidden_xfc_shape[0],\n",
    "                                                          1,\n",
    "                                                        self.hidden_x2_shape[1]*self.hidden_x2_shape[2]*self.hidden_x2_shape[3],\n",
    "                                                        self.hidden_x2_shape[1]*self.hidden_x2_shape[2]*self.hidden_x2_shape[3]\n",
    "                                                         )).shape\n",
    "        # bp()\n",
    "        # Flatten layer\n",
    "        self.flatten_layer = nn.Flatten()\n",
    "        # bp()\n",
    "        # Final embedding block - Output 4 values - linear\n",
    "        in_shape = self.hidden_x2_shape[1] * self.hidden_x2_shape[2] * self.hidden_x2_shape[3]\n",
    "        self.hidden_embedding = nn.Sequential(\n",
    "            nn.Linear(in_shape \n",
    "                      ,100),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(100, 50),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(50, self.dense_params)\n",
    "        )\n",
    "        # bp()\n",
    "    def forward(self, x, n=-1):\n",
    "        # x = x['input'].type(torch.FloatTensor).to(self.device)\n",
    "        # bp()\n",
    "        x = self.hidden_x1(x)\n",
    "        # bp()\n",
    "        xfc = torch.reshape(x, (x.shape[0], -1))  # batch size, features\n",
    "        # bp()\n",
    "        xfc = self.hidden_xfc(xfc)\n",
    "        # bp()\n",
    "        x = self.hidden_x2(x)\n",
    "        # bp()\n",
    "        # x=self.flatten_layer(x)\n",
    "        # bp()\n",
    "        # x=x.unsqueeze(1).repeat(1,x.shape[-1],1)\n",
    "        # # bp()\n",
    "        # # print(x.shape)\n",
    "        # x = self.hidden_x3(x.unsqueeze(1))\n",
    "        # bp()\n",
    "        # print(x.shape)\n",
    "        cnn_flat = self.flatten_layer(x)\n",
    "        # bp()\n",
    "        # encoded = torch.cat((cnn_flat, xfc), 1)  # merge dense and 1d conv layers\n",
    "        # print(\"encoded\", encoded.shape)    \n",
    "            \n",
    "        embedding = self.hidden_embedding(cnn_flat)  # output is 4 parameters\n",
    "\n",
    "        # print(\"embedding\", embedding.shape)    \n",
    "        \n",
    "        embedding = torch.reshape(embedding, (embedding.shape[0], 2, -1,4))\n",
    "        embedding = embedding.transpose(2,3)\n",
    "        # bp()\n",
    "        # embedding[:,:,0,:] = self.sigmoid(embedding[:,:,0,:]) #\n",
    "        embedding[:,:,0,:] = embedding[:,:,0,:] * self.model.mean_sd.repeat(3,1).T.to(self.device) + self.model.mean_mean.repeat(3,1).T.to(self.device)\n",
    "        # embedding[:,:,1,:] = self.sigmoid(embedding[:,:,1,:]) # \n",
    "        embedding[:,:,1,:] = embedding[:,:,1,:] * self.model.sd_sd.repeat(3,1).T.to(self.device) + self.model.sd_mean.repeat(3,1).T.to(self.device)\n",
    "        # embedding[:,:,2,:] = self.sigmoid(embedding[:,:,2,:]) # \n",
    "        embedding[:,:,2,:] = embedding[:,:,2,:] * self.model.amp_sd.repeat(3,1).T.to(self.device) + self.model.amp_mean.repeat(3,1).T.to(self.device)\n",
    "        embedding[:,:,3,:] = self.sigmoid(embedding[:,:,3,:]) #* self.model.fraction_sd + self.model.fraction_mean\n",
    "        # bp()\n",
    "        # embedding = torch.transpose(embedding,2,3)\n",
    "        embedding = embedding.unsqueeze(1)\n",
    "        embedding = torch.abs(embedding)\n",
    "        self.embed = embedding\n",
    "        # bp()\n",
    "        out = self.model.compute(embedding, device = self.device)\n",
    "        # bp()\n",
    "        # out = torch.atleast_3d(out) #makes out 3 demensional view of each 0 dimensions in input out\n",
    "        \n",
    "        return out.to(self.device), embedding.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(checkpoint,foldername,epoch,loss):\n",
    "    '''\n",
    "    Save pkl file to a given folder and adds date and epoch\n",
    "    \n",
    "    checkpoint (str): pkl file to save\n",
    "    foldername (str): folder to save file in \n",
    "    epoch (int): \n",
    "    '''\n",
    "    today = date.today()\n",
    "    save_date=today.strftime('(%Y-%m-%d)')\n",
    "    # make_folder(foldername)\n",
    "    torch.save(checkpoint, f'./{foldername}/{save_date}_epoch:{epoch:05d}_loss:{loss:.5f}_weights.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 100,
     "status": "ok",
     "timestamp": 1669492920471,
     "user": {
      "displayName": "Xinqiao Zhang",
      "userId": "17385836557717323900"
     },
     "user_tz": 300
    },
    "id": "KNN6DGt4c4rL"
   },
   "outputs": [],
   "source": [
    "constructor = PseudoVoigt_2D( mean=[[2,8],[5,7]], \n",
    "                              sd=[[.5,2],[1,3]],\n",
    "                              amp=[[2,6],[2,6]], \n",
    "                              fraction = [[0,0],[0,0]],\n",
    "                            x_vector = torch.linspace(0,10,100), size=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1669493674930,
     "user": {
      "displayName": "Xinqiao Zhang",
      "userId": "17385836557717323900"
     },
     "user_tz": 300
    },
    "id": "6l0I9EKjc4rO"
   },
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# takes ab 5 s to initialize model\n",
    "# x_vector = torch.linspace(0,10,100) #evenly distributes values in a tensor from 1 to 10 with 100 steps\n",
    "loss_func = torch.nn.MSELoss()\n",
    "model = DensePhysLarger(x_vector, constructor, dense_params=12*2, model_params=4, verbose=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "\n",
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:20<00:00, 12.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0/50, recon loss = 3.25459356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:19<00:00, 12.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/50, recon loss = 1.75211914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:19<00:00, 12.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 2/50, recon loss = 1.43864106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:19<00:00, 12.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 3/50, recon loss = 1.32866099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:19<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 4/50, recon loss = 1.27052862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5/50, recon loss = 1.18451885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:16<00:00, 13.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 6/50, recon loss = 1.12884778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:17<00:00, 12.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 7/50, recon loss = 1.07528817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:17<00:00, 12.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 8/50, recon loss = 1.05707231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:17<00:00, 12.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 9/50, recon loss = 1.03559972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:17<00:00, 12.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 10/50, recon loss = 1.03156535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 11/50, recon loss = 1.01908725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 12/50, recon loss = 1.00139511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:17<00:00, 12.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 13/50, recon loss = 1.00571296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 14/50, recon loss = 0.99877150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 15/50, recon loss = 0.99832890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 16/50, recon loss = 0.99563447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:17<00:00, 12.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 17/50, recon loss = 0.98089212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:16<00:00, 13.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 18/50, recon loss = 0.99138134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:17<00:00, 12.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 19/50, recon loss = 0.98042864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 20/50, recon loss = 0.98500014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:17<00:00, 12.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 21/50, recon loss = 0.98641459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 22/50, recon loss = 0.98701970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:17<00:00, 12.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 23/50, recon loss = 0.97637414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 24/50, recon loss = 0.97414862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:16<00:00, 13.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 25/50, recon loss = 0.97578285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:16<00:00, 13.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 26/50, recon loss = 0.97019625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:16<00:00, 13.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 27/50, recon loss = 0.96929989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:17<00:00, 12.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 28/50, recon loss = 0.96607272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:17<00:00, 12.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 29/50, recon loss = 0.96755276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:17<00:00, 12.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 30/50, recon loss = 0.96859991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:17<00:00, 12.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 31/50, recon loss = 0.96532033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:17<00:00, 12.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 32/50, recon loss = 0.96189284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:16<00:00, 13.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 33/50, recon loss = 0.97080752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:17<00:00, 12.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 34/50, recon loss = 0.96211172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:16<00:00, 13.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 35/50, recon loss = 0.96495839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:16<00:00, 13.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 36/50, recon loss = 0.95679239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:17<00:00, 12.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 37/50, recon loss = 0.95735208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 38/50, recon loss = 0.95765377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:17<00:00, 12.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 39/50, recon loss = 0.95328191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:17<00:00, 12.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 40/50, recon loss = 0.94958192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:17<00:00, 12.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 41/50, recon loss = 0.95085765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:16<00:00, 12.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 42/50, recon loss = 0.95445916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:17<00:00, 12.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 43/50, recon loss = 0.94870658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:16<00:00, 13.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 44/50, recon loss = 0.94598401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:17<00:00, 12.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 45/50, recon loss = 0.95744216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:17<00:00, 12.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 46/50, recon loss = 0.94662032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:17<00:00, 12.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 47/50, recon loss = 0.95473710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:17<00:00, 12.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 48/50, recon loss = 0.95402888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:16<00:00, 13.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 49/50, recon loss = 0.94521699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start,epochs=0,50 # define how many epochs\n",
    "save_folder = '2D_results'\n",
    "for epoch in range(start,epochs):\n",
    "    train_loss = 0.\n",
    "    total_num = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in tqdm(dataloader):\n",
    "        # train_batch = torch.FloatTensor(batch['input'].transpose(1,2))\n",
    "        train_batch = batch['input'].type(torch.FloatTensor).cuda()\n",
    "        pred, _ = model(train_batch.cuda())\n",
    "        # bp()\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_func(train_batch, pred)\n",
    "        loss.backward(create_graph=False)\n",
    "        train_loss += loss.item() * pred.shape[0]\n",
    "        total_num += pred.shape[0]\n",
    "        optimizer.step()\n",
    "        \n",
    "    # bp()\n",
    "    train_loss /= total_num\n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        # 'params': parameters_dict,\n",
    "        'train_loss': train_loss,\n",
    "        # 'seed': seed\n",
    "    }\n",
    "    \n",
    "    if (epoch % 5 == 0):\n",
    "        save_model(checkpoint,save_folder,epoch,train_loss)\n",
    "        \n",
    "    print(f\"epoch : {epoch}/{epochs}, recon loss = {train_loss:.8f}\")\n",
    "    # print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './2D_results/(2022-12-01)_epoch:00045_loss:0.95744_weights.pkl' # fill with your saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(file_path)  \n",
    "\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = DataLoader(data_test, batch_size=batch_size,\n",
    "#                         shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.58 ms ± 149 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "pred,par = model(batch['input'].type(torch.FloatTensor).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input (32, 1, 100, 100)\n",
      "params (32, 1, 2, 4, 3)\n",
      "outpu:  torch.Size([32, 1, 100, 100])\n",
      "parameter:  torch.Size([32, 1, 2, 4, 3])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54daf48f87ec418f99869d54a17b22cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=15, description='i', max=31), Output()), _dom_classes=('widget-interact'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.f(i)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "\n",
    "x = batch['input'].cpu().detach().numpy()\n",
    "p = batch['params'].cpu().detach().numpy()\n",
    "true = constructor.compute(batch['params'])\n",
    "print('input', x.shape)\n",
    "print('params', p.shape)\n",
    "\n",
    "pred,par = model(batch['input'].type(torch.FloatTensor).cuda())\n",
    "print('outpu: ',pred.shape)\n",
    "print('parameter: ', par.shape)\n",
    "x_guess=pred.cpu().detach().numpy()\n",
    "p_guess=par.cpu().detach().numpy()\n",
    "\n",
    "def f(i):\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(x[i,0,:,:])\n",
    "    plt.colorbar()\n",
    "    plt.suptitle(f'noise = 3')\n",
    "    plt.xticks(ticks = np.linspace(0,99,11),labels = np.linspace(0,10,11))\n",
    "    plt.yticks(ticks = np.linspace(0,99,11),labels = np.linspace(0,10,11))\n",
    "    plt.scatter(p_guess[i,0,0,0]*10,p_guess[i,0,1,0]*10,c='black',marker='x',label='pred') # plot means\n",
    "    plt.scatter(p[i,0,0,0]*10,p[i,0,1,0]*10,c='r',marker='x',label='true') # plot means\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(true[i,0,:,:])\n",
    "    plt.colorbar()\n",
    "    plt.suptitle(f'true noiseless')\n",
    "    plt.xticks(ticks = np.linspace(0,99,11),labels = np.linspace(0,10,11))\n",
    "    plt.yticks(ticks = np.linspace(0,99,11),labels = np.linspace(0,10,11))\n",
    "    plt.scatter(p_guess[i,0,0,0]*10,p_guess[i,0,1,0]*10,c='black',marker='x',label='pred') # plot means\n",
    "    plt.scatter(p[i,0,0,0]*10,p[i,0,1,0]*10,c='r',marker='x',label='true') # plot means\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(x_guess[i,0,:,:])\n",
    "    plt.colorbar()\n",
    "    plt.suptitle(f'output spectrum and predicted means')\n",
    "    plt.xticks(ticks = np.linspace(0,99,11),labels = np.linspace(0,10,11))\n",
    "    plt.yticks(ticks = np.linspace(0,99,11),labels = np.linspace(0,10,11))\n",
    "    plt.scatter(p_guess[i,0,0,0]*10,p_guess[i,0,1,0]*10,c='black',marker='x',label='pred') # plot means\n",
    "    plt.scatter(p[i,0,0,0]*10,p[i,0,1,0]*10,c='r',marker='x',label='true') # plot means\n",
    "    plt.show()\n",
    "    \n",
    "interact(f,i=(0,batch_size-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
